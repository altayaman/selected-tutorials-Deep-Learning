<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<!-- saved from url=(0052)http://users.sussex.ac.uk/~christ/crs/ml/lec02b.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><style type="text/css">
body { font-family: Arial }
</style>
<title> Machine Learning - Lecture 4: The Naive Bayes Classifier</title></head>
<body text="#1A0D99" bgcolor="#FFFFFF" link="#E61A1A" alink="#FF0000">
<h1><center> Machine Learning - Lecture 4: The Naive Bayes Classifier</center></h1>
<font size="+2"><center><i>Chris Thornton</i></center></font><p></p><hr>
<h1>Sample ML task</h1>


<pre>  SYMPTOM       OCCUPATION    AILMENT

  sneezing      nurse         flu
  sneezing      farmer        hayfever
  headache      builder       concussion
  headache      builder       flu
  sneezing      teacher       flu
  headache      teacher       concussion
</pre>


<pre>  sneezing      builder       ???
</pre>
  <p>

What ailment should we predict for a sneezing builder and
why?</p><p>


</p><h1>Introduction</h1>

Clustering and nearest-neighbour methods are ideally suited
for use with numeric data.<p>

However, data often use using categorical values, i.e.,
names or symbols.</p><p>

In this situation, it may be better to use a probabilistic
method, such as the Naive Bayes Classifier (NBC).</p><p>

</p><h1>Probabilities</h1>

Let's say we have some data that lists symptoms and
ailments for everybody in a certain group.<p>

</p><pre>  SYMPTOM       AILMENT

  sneezing      flu
  sneezing      hayfever
  headache      concussion
  sneezing      flu
  coughing      flu
  backache      none
  vomiting      concussion
  crying        hayfever
  temperature   flu
  drowsiness    concussion
</pre>


<h1>Prediction</h1>

There are 10 cases in all so we can work out the probability
of seeing a particular ailment or symptom just by counting
and dividing by 10.<p>

</p><pre>  P(hayfever) = 2/10 = 0.2

  P(vomiting) = 1/10 = 0.1
</pre>

As a simple, statistical model of the data, these (so-called
<b>prior</b>) probabilities can be used for prediction.<p>

Let's say we're undecided whether someone has flu or
hayfever.</p><p>

We can use the fact that P(flu) &gt; P(hayfever) to predict
it's more likely to be flu.</p><p>


</p><h1>Conditional probabilities</h1>

This sort of modeling becomes more useful when <b>conditional
probabilities</b> are used.<p>

These are values we work out by looking at the probability
of seeing one value given we see another, e.g., the
probability of vomiting given concussion.</p><p>

Conditional probabilites are notated using the bar `|' to
separate the <b>conditioned</b> from the <b>conditioning</b>
value.</p><p>

The probability of vomiting given concussion is written </p><p>

</p><pre>  P(vomiting|concussion)
</pre>

We can work this value out by seeing what proportion of the
cases involving concussion also show vomiting.<p>

</p><pre>  P(vomiting|concussion) = 1/3 = 0.3333
</pre>

<h1>Prediction from conditional probabilities</h1>

Conditional probabilities enable conditional <i>predictions</i>.<p>

For example, we could tell someone who's known to have
concussion that there's a 1/3 chance of them vomiting.</p><p>

This can also be a way of generating diagnoses.</p><p>

If someone reports they've been sneezing a lot, we can say
there's a 2/3 chance of them having flu, since</p><p>

</p><pre>  P(flu|sneezing) = 2/3
</pre>

With slightly less likelihood (1/3) we could say they have
hayfever, since<p>

</p><pre>  P(hayfever|sneezing) = 1/3
</pre>


<h1>The problem of multiple attributes</h1>

What happens if the data include more than one symptom?<p>

We might have something like this.</p><p>

</p><pre>  SYMPTOM       OCCUPATION    AILMENT

  sneezing      nurse         flu
  sneezing      farmer        hayfever
  headache      builder       concussion
</pre>

We'd like to be able to work out probabilities conditional
on multiple symptoms, e.g.,<p>

</p><pre>  P(flu|sneezing,builder)
</pre>


But if a combination doesn't appear in the data, how do we
calculate its conditional probability?<p>


</p><h1>Using inversion</h1>

There's no way to sample a probability conditional on a
combination that doesn't appear. <p>

But we can work it out by looking at probabilities that do
appear.</p><p>

Observable probabilities that contribute to</p><p>

</p><pre>  P(flu|sneezing,builder)
</pre>

are <p>

</p><pre>  P(flu)
  P(sneezing|flu)
  P(builder|flu)
</pre>

All we need is some way of putting these together.<p>

</p><h1>The naive assumption</h1>

Probability theory says that if several factors don't depend
on each other in any way, the probability of seeing them
together is just the product of their probabilities.<p>

So assuming that sneezing has no impact on whether you're a
builder, we can say that</p><p>

</p><pre>  P(sneezing,builder|flu) = P(sneezing|flu)P(builder|flu)
</pre>

The probability of a sneezing builder having flu must
depend on the chances of this combination of attributes
indicating flu. So<p>

</p><pre>  P(flu|sneezing,builder)
</pre>

must be proportional to  <p>

</p><pre>  P(flu)P(sneezing,builder|flu)
</pre>


<h1>Normalization needed</h1>

Unfortunately, this value is purely based on cases of flu.
It doesn't take into account how common this ailment is.<p>

We need to factor in the probability of this combination of
attributes associating with flu in particular, rather than
some other ailment.</p><p>

We do this by expressing the value in proportion to the
probability of seeing the combination of attributes.</p><p>

</p><center>
<img src="./Machine Learning - Lecture 4_ The Naive Bayes Classifier_files/lec02b-report-f1.png">
</center><br>

This gives us the value we want.<p>

</p><h1>The answer</h1>

Assemble all the constituents needed<p>


 P(flu) = 0.5<br>
 P(sneezing|flu)=0.66<br>
 P(builder|flu)=0.33<br>
 P(sneezing,builder|flu)=(0.66x0.33)=0.22<br>
 P(sneezing)=0.5<br>
 P(builder)=0.33<br>
 P(sneezing,builder)=(0.5x0.33)=0.165<br>


Plug values into the formula: </p><p>

</p><center>
<img src="./Machine Learning - Lecture 4_ The Naive Bayes Classifier_files/lec02b-report-f2.png">
</center><br>


It turns out the sneezing builder has flu with probability
0.66.<p>


</p><h1>Bayes rule</h1>

What we've worked out here is just an application of
<b>Bayes rule</b>, the standard formula for inverting
conditional probabilities.<p>

</p><center>
<img src="./Machine Learning - Lecture 4_ The Naive Bayes Classifier_files/lec02b-report-f3.png">
</center><br>

We've looked at ailments and symptoms, but the
method can be used whenever we need <b>classifications</b> of
cases described in terms of <b>attributes</b>.<p>

The more general version of Bayes rule deals with the
case where <img src="./Machine Learning - Lecture 4_ The Naive Bayes Classifier_files/lec02b-report-f4.png"> is a class value, and the
attributes are <img src="./Machine Learning - Lecture 4_ The Naive Bayes Classifier_files/lec02b-report-f5.png">.</p><p>


</p><center>
<img src="./Machine Learning - Lecture 4_ The Naive Bayes Classifier_files/lec02b-report-f6.png">
</center><br>


<h1>Naive Bayes Classifier</h1>


A Naive Bayes Classifier is a program which predicts a class
value given a set of set of attributes.<p>

For each known class value, </p><p>

</p><ol>
<li> Calculate probabilities for each attribute, conditional
on the class value.<p>

</p></li><li> Use the product rule to obtain a joint conditional
probability for the attributes.<p>

</p></li><li> Use Bayes rule to derive conditional probabilities for
the class variable.<p>
</p></li></ol>

Once this has been done for all class values, output the
class with the highest probability.<p>


</p><h1>The problem of missing combinations</h1>

A niggling problem with the NBC is where the dataset
doesn't provide one or more of the probabilities we need.<p>

We then get a probability of zero factored into
the mix.</p><p>

This may cause us to divide by zero, or simply make the
final value itself zero.</p><p>

The easiest solution is to ignore zero-valued probabilities
altogether if we can.</p><p>


</p><h1>Idiot's Bayes?</h1>

Statisticians are somewhat disturbed by use of the NBC
(which they dub <b>Idiot's Bayes</b>) because the naive
assumption of independence is almost always invalid in the
real world.<p>

However, the method has been shown to perform surprisingly
well in a wide variety of contexts.</p><p>

Research continues on why this is.</p><p>

</p><h1>Summary</h1>

<ul>
<li> Clustering and nearest-neighbour methods ideally suited
to numeric data.<p>

</p></li><li> Probablistic modeling may be more effective with
categorical (symbolic) data.<p>

</p></li><li> Probabilities easily derived from datasets.<p>

</p></li><li> But for classification, we normally need to invert the
conditional probabilities we can sample.<p>

</p></li><li> The Naive Bayes Classifier uses Bayes Rule to identify
the class with the highest probability.<p>

</p></li><li> On average, the NBC seems to be perform better than
expected.<p>
</p></li></ul>


<h1>Questions</h1>

<ul>
<li> What domain do the probabilities we derive from a dataset
apply to?<p>

</p></li><li> What is the difference between a condition<i>ing</i> and a
condition<i>ed</i> value in a defined probability?<p>

</p></li><li> Where should we place the conditioned value in a
conditional probability statement?<p>

</p></li><li> What sort of modeling process is involved in the NBC?<p>

</p></li><li> Where we have just one class, and one attribute variable,
we can work out all conditional probabilities directly from
the dataset. Why is this more difficult with more than one
attribute?<p>

</p></li><li> Identify two attributes that are certainly independent,
two that are certainly dependent, and two that are somewhere
in between.

</li></ul>



</body></html>