
<!-- saved from url=(0043)http://www.saedsayad.com/naive_bayesian.htm -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta name="GENERATOR" content="Microsoft FrontPage 4.0">
<meta name="ProgId" content="FrontPage.Editor.Document">
<title>Naive Bayesian</title>
<link rel="icon" type="image/png" href="http://www.saedsayad.com/logosmart.png">
<script type="text/javascript" async="" src="./Naive Bayesian_files/ga.js"></script><script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-20171535-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>

<body>

<table border="0" width="800">
  <tbody><tr>
    <td><font face="Calibri"><a href="http://www.saedsayad.com/data_mining_map.htm">Map</a>
      &gt; <a href="http://www.saedsayad.com/data_mining.htm">Data
      Mining</a> &gt; <a href="http://www.saedsayad.com/predicting_the_future.htm"> Predicting the Future</a> &gt;
      <a href="http://www.saedsayad.com/modeling.htm"> Modeling</a> &gt;
      <a href="http://www.saedsayad.com/classification.htm">
      Classification</a> &gt; Naive Bayesian</font></td>
  </tr>
  <tr>
    <td>
      <font face="Calibri" color="#008000">&nbsp;</font>
    </td>
  </tr>
  <tr>
    <td>
      <h3 align="center"><font face="Calibri" color="#008000">Naive Bayesian</font></h3>
    </td>
  </tr>
  <tr>
    <td><font face="Calibri">The Naive Bayesian classifier is based on Bayes’ theorem with independence assumptions between predictors. A Naive Bayesian model is easy to build, with no complicated iterative
      parameter estimation which makes it particularly useful for very large
      datasets. Despite its simplicity, the Naive Bayesian classifier often does surprisingly well and is widely used because it often outperforms more sophisticated classification methods.&nbsp;</font></td>
  </tr>
  <tr>
    <td>
      &nbsp;</td>
  </tr>
  <tr>
    <td>
      <b><font face="Calibri">Algorithm</font></b></td>
  </tr>
  <tr>
    <td>
      <font face="Calibri"> Bayes theorem provides a way of calculating the posterior probability,
      </font><i>P</i>(<i>c|x</i>)<font face="Calibri">, from </font><i>P</i>(<i>c</i>)<font face="Calibri">,
      </font><i>P</i>(<i>x</i>)<font face="Calibri">, and </font><i> P</i>(<i>x|c</i>)<font face="Calibri">.
      Naive Bayes classifier assume that the effect of the value of a predictor (</font><i>x</i><font face="Calibri">)
      on a given class (</font><i>c</i><font face="Calibri">) is independent of the values of other
      predictors. This assumption is called class conditional independence.</font></td>
  </tr>
  <tr>
    <td>
      <p align="center"><img border="0" src="./Naive Bayesian_files/Bayes_rule.png" width="461" height="264"></p></td>
  </tr>
  <tr>
    <td>
      <ul>
        <li><i>P</i>(<i>c|x</i>)<font face="Calibri"> is the posterior probability of
      </font><i>class</i><font face="Calibri">  (<i>target</i>) given </font><i>predictor</i><font face="Calibri">
          (<i>attribute</i>).&nbsp;</font></li>
        <li><i>P</i>(<i>c</i>)<font face="Calibri"> is the prior
      probability of </font><i>class</i><font face="Calibri">.&nbsp;</font></li>
        <li><i>P</i>(<i>x|c</i>)<font face="Calibri"> is
      the likelihood which is the probability of </font><i>predictor</i><font face="Calibri">
          given </font><i>class</i><font face="Calibri">.&nbsp;</font></li>
        <li><i>P</i>(<i>x</i>)<font face="Calibri"> is the prior probability of
      </font><i>predictor</i><font face="Calibri">.</font></li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td><i>Example</i>:</td>
  </tr>
  <tr>
    <td>
      <font face="Calibri">The posterior probability can be calculated by first,
      constructing a frequency table for each
      attribute against the target. Then, transforming the frequency tables to
      likelihood tables and finally use the Naive Bayesian equation to calculate
      the posterior probability for each class. The class with the highest
      posterior probability is the outcome of prediction.&nbsp;</font></td>
  </tr>
  <tr>
    <td>
      <p align="center"><img border="0" src="./Naive Bayesian_files/Bayes_3.png" width="669" height="309"></p></td>
  </tr>
  <tr>
    <td><font face="Calibri"><b>&nbsp;</b></font></td>
  </tr>
  <tr>
    <td><font face="Calibri"><b>The zero-frequency problem</b></font></td>
  </tr>
  <tr>
    <td><font face="Calibri">Add 1 to the count for every attribute value-class combination
      (<i>Laplace estimator</i>) when an attribute value (<i>Outlook=Overcast</i>)
      doesn’t occur with every class value (<i>Play Golf=no</i>).</font></td>
  </tr>
  <tr>
    <td>&nbsp;&nbsp;</td>
  </tr>
  <tr>
    <td><font face="Calibri"><b>Numerical Predictors</b></font></td>
  </tr>
  <tr>
    <td><font face="Calibri">Numerical variables need to be transformed to their
      categorical counterparts (<a href="http://www.saedsayad.com/binning.htm">binning</a>) before
      constructing their frequency tables. The other option we have is using the
      distribution of the numerical variable to have a good guess of the
      frequency. For example, one common practice is to assume normal distributions for numerical
      variables.</font></td>
  </tr>
  <tr>
    <td><font face="Calibri">&nbsp;</font></td>
  </tr>
  <tr>
    <td><font face="Calibri">The probability density function for the normal
      distribution is defined by two parameters (mean and standard deviation).</font></td>
  </tr>
  <tr>
    <td>
      <p align="center"><img border="0" src="./Naive Bayesian_files/Bayes_NormDist.png"></p></td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td><i>Example</i>:</td>
  </tr>
  <tr>
    <td align="center">
      <table>
        <colgroup><col width="64" span="2" style="width:48pt">
        <col width="21" span="9" style="mso-width-source:userset;mso-width-alt:768;
 width:16pt">
        <col width="47" style="mso-width-source:userset;mso-width-alt:1718;width:35pt">
        <col width="64" span="2" style="width:48pt">
        </colgroup><tbody><tr height="20" style="height:15.0pt">
          <td height="20" width="64" style="height:15.0pt;width:48pt"></td>
          <td width="64" style="width:48pt"></td>
          <td colspan="8" class="xl65" width="168" style="width:128pt">
            <p align="center"><font face="Calibri"><b>Humidity</b></font></p>
          </td>
          <td width="21" style="width:16pt"></td>
          <td class="xl65" width="64" style="width:48pt">
            <p align="right"><font face="Calibri"><i>Mean</i></font></p>
          </td>
          <td class="xl65" width="64" style="width:48pt">
            <p align="right"><font face="Calibri"><i>StDev</i></font></p>
          </td>
        </tr>
        <tr height="20" style="height:15.0pt">
          <td rowspan="2" height="40" class="xl66" style="height:30.0pt"><font face="Calibri"><b>Play
            Golf</b></font></td>
          <td>
            <p align="center"><font face="Calibri">yes</font></p>
          </td>
          <td align="right"><font face="Calibri">86</font></td>
          <td align="right"><font face="Calibri">96</font></td>
          <td align="right"><font face="Calibri">80</font></td>
          <td align="right"><font face="Calibri">65</font></td>
          <td align="right"><font face="Calibri">70</font></td>
          <td align="right"><font face="Calibri">80</font></td>
          <td align="right"><font face="Calibri">70</font></td>
          <td align="right"><font face="Calibri">90</font></td>
          <td align="right"><font face="Calibri">75</font></td>
          <td class="xl67" align="right"><font face="Calibri">79.1</font></td>
          <td class="xl67" align="right"><font face="Calibri">10.2</font></td>
        </tr>
        <tr height="20" style="height:15.0pt">
          <td height="20" style="height:15.0pt">
            <p align="center"><font face="Calibri">no</font></p>
          </td>
          <td align="right"><font face="Calibri">85</font></td>
          <td align="right"><font face="Calibri">90</font></td>
          <td align="right"><font face="Calibri">70</font></td>
          <td align="right"><font face="Calibri">95</font></td>
          <td align="right"><font face="Calibri">91</font></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td class="xl67" align="right"><font face="Calibri">86.2</font></td>
          <td class="xl67" align="right"><font face="Calibri">9.7</font></td>
        </tr>
      </tbody></table>
    </td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>
      <p align="center"><img border="0" src="./Naive Bayesian_files/Bayes_NormDist_1.png"></p></td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td><font face="Calibri"><b>Predictors Contribution</b></font></td>
  </tr>
  <tr>
    <td><font face="Calibri">Kononenko's <i> information gain</i> as a sum of information contributed by each
      attribute can offer an explanation on how values of the predictors influence the
      class probability.</font></td>
  </tr>
  <tr>
    <td>
      <p align="center"><img border="0" src="./Naive Bayesian_files/Kononenko.png" width="223" height="62"></p></td>
  </tr>
  <tr>
    <td><font face="Calibri">The contribution of predictors can also be visualized
      by plotting <a href="http://www.saedsayad.com/further_readings.htm"><i>nomograms</i></a>. Nomogram plots log odds ratios for each value of each
      predictor. Lengths of the lines correspond to spans of odds ratios, suggesting importance of
      the related predictor. It also shows impacts of individual values of the
      predictor.</font></td>
  </tr>
  <tr>
    <td>
      <p align="center"><img border="0" src="./Naive Bayesian_files/Nomogram.png" width="403" height="172"></p></td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>
      <table border="0" width="100%">
        <tbody><tr>
          <td width="8%"><font face="Calibri"><a href="http://www.saedsayad.com/naive_bayesian_exercise.htm"><span style="background-color: #CCFFFF">Exercise</span></a></font>
          </td>
          <td width="7%"><a href="http://www.saedsayad.com/datasets/Bayes.txt" target="_blank"><img border="0" src="./Naive Bayesian_files/R.png" width="40" height="31"></a></td>
          <td width="85%"><img border="0" src="./Naive Bayesian_files/flash16c.png" width="16" height="16">
      <font face="Calibri"><a href="http://www.saedsayad.com/flash/Bayesian.html" target="_blank">Naive
      Bayesian Interactive</a></font>
          </td>
        </tr>
      </tbody></table>
    </td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td><font face="Calibri"><img border="0" src="./Naive Bayesian_files/invent.png" width="27" height="24">
      Try to invent a real time Bayesian classifier. You should be able to add
      or remove data and variables (predictors and classes) on the fly.</font>
    </td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>
    </td>
  </tr>
  <tr>
    <td></td>
  </tr>
</tbody></table>




</body></html>