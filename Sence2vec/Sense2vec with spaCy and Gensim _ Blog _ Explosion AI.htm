<!DOCTYPE html>
<!-- saved from url=(0046)https://explosion.ai/blog/sense2vec-with-spacy -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>Sense2vec with spaCy and Gensim | Blog | Explosion AI</title><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="referrer" content="always"><meta name="google-site-verification" content="g_Q1d9lRu0QfxU40-Y2oU9lZbWcXOkpN62Eb_1mTj94"><link rel="shortcut icon" href="https://explosion.ai/assets/img/favicon.ico"><link rel="alternate" type="application/rss+xml" title="RSS" href="https://explosion.ai/feed.xml"><link rel="stylesheet" href="./Sense2vec with spaCy and Gensim _ Blog _ Explosion AI_files/style.css"><link rel="stylesheet" href="./Sense2vec with spaCy and Gensim _ Blog _ Explosion AI_files/323b37"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@explosion_ai"><meta name="twitter:title" content="Sense2vec with spaCy and Gensim"><meta name="twitter:description" content="If you were doing text analytics in 2015, you were probably using word2vec. Sense2vec (Trask et. al, 2015) is a new twist on word2vec that lets you learn more interesting, detailed and context-sensitive word vectors. This post motivates the idea, explains our implementation, and comes with an interactive demo that we&#39;ve found surprisingly addictive."><meta name="twitter:image" content="https://explosion.ai/blog/img/sense2vec.jpg"><meta property="og:type" content="website"><meta property="og:site_name" content="Explosion AI"><meta property="og:url" content="https://explosion.ai/blog/sense2vec-with-spacy"><meta property="og:title" content="Sense2vec with spaCy and Gensim"><meta property="og:description" content="If you were doing text analytics in 2015, you were probably using word2vec. Sense2vec (Trask et. al, 2015) is a new twist on word2vec that lets you learn more interesting, detailed and context-sensitive word vectors. This post motivates the idea, explains our implementation, and comes with an interactive demo that we&#39;ve found surprisingly addictive."><meta property="og:image" content="https://explosion.ai/blog/img/sense2vec.jpg"></head><body><header class="flex items-start justify-between bg px2 pt2"><a href="https://explosion.ai/" aria-label="Home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 500 500" width="40" height="40" fill="currentColor" style="width: 3.5rem; height: 3.5rem" class="hov-rotate"><path d="M111.7 74.9L91.2 93.1l9.1 10.2 17.8-15.8 7.4 8.4-17.8 15.8 10.1 11.4 20.6-18.2 7.7 8.7-30.4 26.9-41.9-47.3 30.3-26.9 7.6 8.6zM190.8 59.6L219 84.3l-14.4 4.5-20.4-18.2-6.4 26.6-14.4 4.5 8.9-36.4-26.9-24.1 14.3-4.5L179 54.2l5.7-25.2 14.3-4.5-8.2 35.1zM250.1 21.2l27.1 3.4c6.1.8 10.8 3.1 14 7.2 3.2 4.1 4.5 9.2 3.7 15.5-.8 6.3-3.2 11-7.4 14.1-4.1 3.1-9.2 4.3-15.3 3.5L258 63.2l-2.8 22.3-13-1.6 7.9-62.7zm11.5 13l-2.2 17.5 12.6 1.6c5.1.6 9.1-2 9.8-7.6.7-5.6-2.5-9.2-7.6-9.9l-12.6-1.6zM329.1 95.4l23.8 13.8-5.8 10L312 98.8l31.8-54.6 11.3 6.6-26 44.6zM440.5 145c-1.3 8.4-5.9 15.4-13.9 21.1s-16.2 7.7-24.6 6.1c-8.4-1.6-15.3-6.3-20.8-14.1-5.5-7.9-7.6-16-6.4-24.4 1.3-8.5 6-15.5 14-21.1 8-5.6 16.2-7.7 24.5-6 8.4 1.6 15.4 6.3 20.9 14.2 5.5 7.6 7.6 15.7 6.3 24.2zM412 119c-5.1-.8-10.3.6-15.6 4.4-5.2 3.7-8.4 8.1-9.4 13.2-1 5.2.2 10.1 3.5 14.8 3.4 4.8 7.5 7.5 12.7 8.2 5.2.8 10.4-.7 15.6-4.4 5.3-3.7 8.4-8.1 9.4-13.2 1.1-5.1-.1-9.9-3.4-14.7-3.4-4.8-7.6-7.6-12.8-8.3zM471.5 237.9c-2.8 4.8-7.1 7.6-13 8.7l-2.6-13.1c5.3-.9 8.1-5 7.2-11-.9-5.8-4.3-8.8-8.9-8.2-2.3.3-3.7 1.4-4.5 3.3-.7 1.9-1.4 5.2-1.7 10.1-.8 7.5-2.2 13.1-4.3 16.9-2.1 3.9-5.7 6.2-10.9 7-6.3.9-11.3-.5-15.2-4.4-3.9-3.8-6.3-9-7.3-15.7-1.1-7.4-.2-13.7 2.6-18.8 2.8-5.1 7.4-8.2 13.7-9.2l2.6 13c-5.6 1.1-8.7 6.6-7.7 13.4 1 6.6 3.9 9.5 8.6 8.8 4.4-.7 5.7-4.5 6.7-14.1.3-3.5.7-6.2 1.1-8.4.4-2.2 1.2-4.4 2.2-6.8 2.1-4.7 6-7.2 11.8-8.1 5.4-.8 10.3.4 14.5 3.7 4.2 3.3 6.9 8.5 8 15.6.9 6.9-.1 12.6-2.9 17.3zM408.6 293.5l2.4-12.9 62 11.7-2.4 12.9-62-11.7zM419.6 396.9c-8.3 2-16.5.3-24.8-5-8.2-5.3-13.2-12.1-14.9-20.5-1.6-8.4.1-16.6 5.3-24.6 5.2-8.1 11.9-13.1 20.2-15.1 8.4-1.9 16.6-.3 24.9 5 8.2 5.3 13.2 12.1 14.8 20.5 1.7 8.4 0 16.6-5.2 24.7-5.2 8-12 13-20.3 15zm13.4-36.3c-1.2-5.1-4.5-9.3-9.9-12.8s-10.6-4.7-15.8-3.7-9.3 4-12.4 8.9-4.1 9.8-2.8 14.8c1.2 5.1 4.5 9.3 9.9 12.8 5.5 3.5 10.7 4.8 15.8 3.7 5.1-.9 9.2-3.8 12.3-8.7s4.1-9.9 2.9-15zM303.6 416.5l9.6-5.4 43.3 20.4-19.2-34 11.4-6.4 31 55-9.6 5.4-43.4-20.5 19.2 34.1-11.3 6.4-31-55zM238.2 468.8c-49 0-96.9-17.4-134.8-49-38.3-32-64-76.7-72.5-125.9-2-11.9-3.1-24-3.1-35.9 0-36.5 9.6-72.6 27.9-104.4 2.1-3.6 6.7-4.9 10.3-2.8 3.6 2.1 4.9 6.7 2.8 10.3-16.9 29.5-25.9 63.1-25.9 96.9 0 11.1 1 22.3 2.9 33.4 7.9 45.7 31.8 87.2 67.3 116.9 35.2 29.3 79.6 45.5 125.1 45.5 11.1 0 22.3-1 33.4-2.9 4.1-.7 8 2 8.7 6.1.7 4.1-2 8-6.1 8.7-11.9 2-24 3.1-36 3.1z"></path></svg></a><nav><ul class="small mt1"><li class="inline pr3 xs-hide sm-hide"><a href="https://explosion.ai/">Home</a></li><li class="inline pr3"><a href="https://explosion.ai/about" class="pb1">About us</a></li><li class="inline pr3"><a href="https://explosion.ai/blog" class="pb1 bold">Blog</a></li><li class="inline pr3"><a href="https://explosion.ai/request" class="pb1">Request</a></li><li class="inline pr2 xs-hide sm-hide"><a href="https://twitter.com/explosion_ai" target="_blank" rel="noopener nofollow" class="a" aria-label="twitter"><svg aria-hidden="true" viewBox="0 0 18 18" width="18" height="18" class="align-middle"><use xlink:href="/assets/img/icons.svg#twitter"></use></svg></a></li><li class="inline pr2 xs-hide sm-hide"><a href="https://github.com/explosion" target="_blank" rel="noopener nofollow" class="a" aria-label="github"><svg aria-hidden="true" viewBox="0 0 18 18" width="18" height="18" class="align-middle"><use xlink:href="/assets/img/icons.svg#github"></use></svg></a></li></ul></nav></header><main><article><figure class="relative overflow-hidden angle angle-top-right"><img src="./Sense2vec with spaCy and Gensim _ Blog _ Explosion AI_files/sense2vec.jpg" width="1440" role="presentation" class="col-12"><figcaption class="rotate-90 absolute bottom-0 right-0 mr2 mb3 px1 bg smallest"><span class="font-reset">©&nbsp;</span><a href="https://dribbble.com/kemal" target="_blank" rel="noopener nofollow" class="a">Kemal Şanlı</a></figcaption></figure><header class="fit pt4 pb3 width-asides"><h1 class="mb2 lh1-25 h1">Sense2vec with spaCy and Gensim</h1><span class="small font-sec upper">by Matthew Honnibal on <time datetime="2016-02-15T00:00:00.000Z">February 15, 2016</time></span></header><progress value="0" max="100" class="js-progress progress fixed top-0 left-0 z4"></progress><section class="fit bg z1 width-asides pb3"><p class="large">If you were doing text analytics in 2015, you were probably using <a href="https://en.wikipedia.org/wiki/Word2vec" target="_blank" rel="noopener nofollow" class="a">word2vec</a>.  Sense2vec <a href="http://arxiv.org/abs/1511.06388" target="_blank" rel="noopener nofollow" class="a">(Trask et. al, 2015)</a> is a new twist on word2vec that lets you learn more interesting, detailed and context-sensitive word vectors.  This post motivates the idea, explains our implementation, and comes with an <a href="https://demos.explosion.ai/sense2vec" target="_blank" class="a">interactive demo</a> that we've found surprisingly <a href="https://demos.explosion.ai/sense2vec/?crack|NOUN" target="_blank" class="a">addictive</a>.</p><h2 class="mb3 lh1-25 h2"><a id="word2vec" href="https://explosion.ai/blog/sense2vec-with-spacy#word2vec" class="permalink relative"><svg aria-hidden="true" viewBox="0 0 24 24" width="24" height="24" class="align-middle permalink-icon"><use xlink:href="/assets/img/icons.svg#dots"></use></svg>Polysemy: the problem with word2vec</a></h2><p>When humans write dictionaries and thesauruses, we define concepts in relation to other concepts.  For automatic natural language processing, it's often more effective to use dictionaries that define concepts in terms of their usage statistics.  The word2vec family of models are the most popular way of creating these dictionaries.  Given a large sample of text, word2vec gives you a dictionary where each definition is just a row of, say, 300 floating-point numbers.  To find out whether two entries in the dictionary are similar, you ask how similar their definitions are — a well-defined mathematical operation.</p><p>The problem with word2vec is the <em>word</em> part.  Consider a word like <em>duck</em>. No individual usage of the word <em>duck</em> refers to the concept "a waterfowl, or the action of crouching".  But that's the concept that word2vec is trying to model — because it smooshes all senses of the words together.  <a href="http://arxiv.org/abs/1511.05392" target="_blank" rel="noopener nofollow" class="a">Nalisnick and Ravi (2015)</a> noticed this problem, and suggested that we should allow word vectors to grow arbitrarily, so that we can do a better job of modelling complicated concepts.  This seems like a nice approach for subtle sense distinctions, but for cases like <em>duck</em> it's not so satisfying.  What we want to do is have different vectors for the different senses.  We also want a simple way of knowing which meaning a given usage refers to.  For this, we need to analyse tokens in context. This is where <a href="https://spacy.io/" target="_blank" class="a">spaCy</a> comes in.</p><h2 class="mb3 lh1-25 h2"><a id="sense2vec" href="https://explosion.ai/blog/sense2vec-with-spacy#sense2vec" class="permalink relative"><svg aria-hidden="true" viewBox="0 0 24 24" width="24" height="24" class="align-middle permalink-icon"><use xlink:href="/assets/img/icons.svg#dots"></use></svg>Sense2vec: Using NLP annotations for more precise vectors</a></h2><p>The idea behind sense2vec is super simple. If the problem is that <em>duck</em> as in <em>waterfowl</em> and <em>duck</em> as in <em>crouch</em> are different concepts, the straight-forward solution is to just have two entries, <a href="https://demos.explosion.ai/sense2vec/?duck|NOUN" target="_blank" class="a">duck<sub>N</sub></a> and <a href="https://demos.explosion.ai/sense2vec/?duck|VERB" target="_blank" class="a">duck<sub>V</sub></a>. We've wanted to try this <a href="https://github.com/spacy-io/spaCy/issues/58" target="_blank" rel="noopener nofollow" class="a">for some time</a>. So when <a href="http://arxiv.org/pdf/1511.06388.pdf" target="_blank" rel="noopener nofollow" class="a">Trask et al (2015)</a> published a nice set of experiments showing that the idea worked well, we were easy to convince.</p><p>We follow Trask et al in adding part-of-speech tags and named entity labels to the tokens.  Additionally, we merge named entities and base noun phrases into single tokens, so that they receive a single vector.  We're very pleased with the results from this, even though we consider the current version an early draft.  There's a lot more that can be done with the idea.  Multi-word verbs such as <em>get up</em> and <em>give back</em> and even <em>take a walk</em> or <em>make a decision</em> would be great extensions.  We also don't do anything currently to trim back phrases that are compositional — phrases which really are two words.</p><p>Here's how the current pre-processing function looks, at the time of writing.  The rest of the code can be found on <a href="https://github.com/spacy-io/sense2vec/" target="_blank" rel="noopener nofollow" class="a">GitHub</a>.</p><pre class="border-left border-thick mb4 o-block language-python"><span class="inline-block small font-sec upper bg-theme px2 py1 mt2">merge_text.py</span><code class="block font-sec lh2 px3 py3 smaller language-python"><span class="token keyword">def</span> <span class="token function">transform_texts</span><span class="token punctuation">(</span>texts<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># Load the annotation models</span>
    nlp <span class="token operator">=</span> English<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># Stream texts through the models. We accumulate a buffer and release</span>
    <span class="token comment" spellcheck="true"># the GIL around the parser, for efficient multi-threading.</span>
    <span class="token keyword">for</span> doc <span class="token keyword">in</span> nlp<span class="token punctuation">.</span>pipe<span class="token punctuation">(</span>texts<span class="token punctuation">,</span> n_threads<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># Iterate over base NPs, e.g. "all their good ideas"</span>
        <span class="token keyword">for</span> np <span class="token keyword">in</span> doc<span class="token punctuation">.</span>noun_chunks<span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># Only keep adjectives and nouns, e.g. "good ideas"</span>
            <span class="token keyword">while</span> len<span class="token punctuation">(</span>np<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">1</span> <span class="token operator">and</span> np<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dep_ <span class="token operator">not</span> <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">'amod'</span><span class="token punctuation">,</span> <span class="token string">'compound'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                np <span class="token operator">=</span> np<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> len<span class="token punctuation">(</span>np<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
                <span class="token comment" spellcheck="true"># Merge the tokens, e.g. good_ideas</span>
                np<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>np<span class="token punctuation">.</span>root<span class="token punctuation">.</span>tag_<span class="token punctuation">,</span> np<span class="token punctuation">.</span>text<span class="token punctuation">,</span> np<span class="token punctuation">.</span>root<span class="token punctuation">.</span>ent_type_<span class="token punctuation">)</span>
            <span class="token comment" spellcheck="true"># Iterate over named entities</span>
            <span class="token keyword">for</span> ent <span class="token keyword">in</span> doc<span class="token punctuation">.</span>ents<span class="token punctuation">:</span>
                <span class="token keyword">if</span> len<span class="token punctuation">(</span>ent<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
                    <span class="token comment" spellcheck="true"># Merge them into single tokens</span>
                    ent<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>ent<span class="token punctuation">.</span>root<span class="token punctuation">.</span>tag_<span class="token punctuation">,</span> ent<span class="token punctuation">.</span>text<span class="token punctuation">,</span> ent<span class="token punctuation">.</span>label_<span class="token punctuation">)</span>
        token_strings <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> token <span class="token keyword">in</span> tokens<span class="token punctuation">:</span>
            text <span class="token operator">=</span> token<span class="token punctuation">.</span>text<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">,</span> <span class="token string">'_'</span><span class="token punctuation">)</span>
            tag <span class="token operator">=</span> token<span class="token punctuation">.</span>ent_type_ <span class="token operator">or</span> token<span class="token punctuation">.</span>pos_
            token_strings<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">'%s|%s'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>text<span class="token punctuation">,</span> tag<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">yield</span> <span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>token_strings<span class="token punctuation">)</span>
</code></pre><p>Even with all this additional processing, we can still train massive models without difficulty. Because spaCy is written in <a href="https://explosion.ai/blog/writing-c-in-cython" class="a">Cython</a>, we can <a href="http://docs.cython.org/src/userguide/parallelism.html" target="_blank" rel="noopener nofollow" class="a">release the GIL</a> around the syntactic parser, allowing efficient multi-threading. With 4 threads, throughput is over 100,000 words per second.</p><p>After pre-processing the text, the vectors can be trained as normal, using <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener nofollow" class="a">the original C code</a>, <a href="https://radimrehurek.com/gensim/" target="_blank" rel="noopener nofollow" class="a">Gensim</a>, or a related technique like <a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener nofollow" class="a">GloVe</a>. So long as it expects the tokens to be whitespace delimited, and sentences to be separated by new lines, there should be no problem.  The only caveat is that the tool should not try to employ its own tokenization — otherwise it might split off our tags.</p><p>We used Gensim, and trained the model using the Skip-Gram with Negative Sampling algorithm, using a frequency threshold of 10 and 5 iterations. After training, we applied a further frequency threshold of 50, to reduce the run-time memory requirements.</p><h2 class="mb3 lh1-25 h2"><a id="examples" href="https://explosion.ai/blog/sense2vec-with-spacy#examples" class="permalink relative"><svg aria-hidden="true" viewBox="0 0 24 24" width="24" height="24" class="align-middle permalink-icon"><use xlink:href="/assets/img/icons.svg#dots"></use></svg>Example queries</a></h2><p>As soon as we started playing with these vectors, we found all sorts of interesting things.Here are some of our first impressions.</p><h3 class="lh1-25 mb2 h3">1. The vector space seems like it'll give a good way to show compositionality:</h3><p>A phrase is <em>compositional</em> to the extent that its meaning is the sum of its parts.  The word vectors give us good insight into this.  The model knows that <em>fair game</em> is not a type of game, while <em>multiplayer game</em> is:</p><pre class="border-left border-thick mb4 o-block language-python"><code class="block font-sec lh2 px3 py3 smaller language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> model<span class="token punctuation">.</span>similarity<span class="token punctuation">(</span><span class="token string">'fair_game|NOUN'</span><span class="token punctuation">,</span> <span class="token string">'game|NOUN'</span><span class="token punctuation">)</span>
<span class="token number">0.034977455677555599</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> model<span class="token punctuation">.</span>similarity<span class="token punctuation">(</span><span class="token string">'multiplayer_game|NOUN'</span><span class="token punctuation">,</span> <span class="token string">'game|NOUN'</span><span class="token punctuation">)</span>
<span class="token number">0.54464530644393849</span>
</code></pre><p>Similarly, it knows that <em>class action</em> is only very weakly a type of action, but a <em>class action lawsuit</em> is definitely a type of lawsuit:</p><pre class="border-left border-thick mb4 o-block language-python"><code class="block font-sec lh2 px3 py3 smaller language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> model<span class="token punctuation">.</span>similarity<span class="token punctuation">(</span><span class="token string">'class_action|NOUN'</span><span class="token punctuation">,</span> <span class="token string">'action|NOUN'</span><span class="token punctuation">)</span>
<span class="token number">0.14957825452335169</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> model<span class="token punctuation">.</span>similarity<span class="token punctuation">(</span><span class="token string">'class_action_lawsuit|NOUN'</span><span class="token punctuation">,</span> <span class="token string">'lawsuit|NOUN'</span><span class="token punctuation">)</span>
<span class="token number">0.69595765453644187</span>
</code></pre><p>Personally, I like the queries where you can see a little of the Reddit shining through (which might not be safe for every workplace). For instance, Reddit understands that a <a href="https://demos.explosion.ai/sense2vec/?garter_snake|NOUN" target="_blank" class="a">garter snake</a> is a type of snake, while a <a href="https://demos.explosion.ai/sense2vec/?trouser_snake|NOUN" target="_blank" class="a">trouser snake</a> is something else entirely.</p><h3 class="lh1-25 mb2 h3">2. Similarity between entities can be kind of fun.</h3><p>Here's what Reddit thinks of Donald Trump:</p><pre class="border-left border-thick mb4 o-block language-python"><code class="block font-sec lh2 px3 py3 smaller language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> model<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'Donald_Trump|PERSON'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">(</span>u<span class="token string">'Sarah_Palin|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.854670465</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">(</span>u<span class="token string">'Mitt_Romney|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.8245523572</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">(</span>u<span class="token string">'Barrack_Obama|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.808201313</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">(</span>u<span class="token string">'Bill_Clinton|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.8045649529</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">(</span>u<span class="token string">'Oprah|GPE'</span><span class="token punctuation">,</span> <span class="token number">0.8042222261</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">(</span>u<span class="token string">'Paris_Hilton|ORG'</span><span class="token punctuation">,</span> <span class="token number">0.7962667942</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">(</span>u<span class="token string">'Oprah_Winfrey|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.7941152453</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">(</span>u<span class="token string">'Stephen_Colbert|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.7926792502</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">(</span>u<span class="token string">'Herman_Cain|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.7869615555</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">(</span>u<span class="token string">'Michael_Moore|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.7835546732</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</code></pre><p>The model is returning entities discussed in similar contexts.  It's interesting to see that the word vectors correctly pick out the idea of Trump as a political figure but also a reality star.  The comparison with <a href="https://en.wikipedia.org/wiki/Michael_Moore" target="_blank" rel="noopener nofollow" class="a">Michael Moore</a> really tickles me. I doubt there are many people who are fans of both.  If I had to pick an odd-one-out, I'd definitely choose Oprah.  That comparison resonates much less with me.</p><p>The entry <code>Oprah|GPE</code> is also quite interesting.  Nobody is living in the United States of Oprah just yet, which is what the tag <code>GPE</code> (geopolitican entity) would imply.  The distributional similarity model has correctly learned that <code>Oprah|GPE</code> is closely related to <code>Oprah_Winfrey|PERSON</code>.  This seems like a promising way to mine for errors made by the named entity recogniser, which could lead to improvements.</p><p>Word2vec has always worked well on named entities.  I find the <a href="https://demos.explosion.ai/sense2vec/?Autechre|PERSON" target="_blank" rel="noopener nofollow" class="a">music region of the vector space</a> particularly satisfying.  It reminds me of the way I used to get music recommendations: by paying attention to the bands frequently mentioned alongside ones I already like.  Of course, now we have much more powerful recommendation models, that look at the listening behaviour of millions of people.  But to me there's something oddly intuitive about many of the band similarities our sense2vec model is returning.</p><p>Of course, the model is far from perfect, and when it produces weird results, it doesn't always pay to think too hard about them. One of our early models "uncovered" a hidden link between Carrot Top and Kate Mara:</p><pre class="border-left border-thick mb4 o-block language-python"><code class="block font-sec lh2 px3 py3 smaller language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> model<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'Carrot_Top|PERSON'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span>u<span class="token string">'Kate_Mara|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.5347248911857605</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'Andy_Samberg|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.5336876511573792</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'Ryan_Gosling|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.5287898182868958</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'Emma_Stone|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.5243821740150452</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'Charlie_Sheen|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.5209298133850098</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'Joseph_Gordon_Levitt|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.5196050405502319</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'Jonah_Hill|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.5151286125183105</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'Zooey_Deschanel|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.514430582523346</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'Gerard_Butler|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.5115377902984619</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'Ellen_Page|PERSON'</span><span class="token punctuation">,</span> <span class="token number">0.5094753503799438</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</code></pre><p>I really spent too long thinking about this.  It just didn't make any sense. Even though it was trivial, it was so bizarre it was almost upsetting.  And then it hit me: is this not the nature of all things Carrot Top?  Perhaps there was a deeper logic to this.  It required further study.  But when we ran the model on more data, and it was gone and soon forgotten.  Just like Carrot Top.</p><h3 class="lh1-25 mb2 h3">3. Reddit talks about food a lot, and those regions of the vector space seem very well defined:</h3><pre class="border-left border-thick mb4 o-block language-python"><code class="block font-sec lh2 px3 py3 smaller language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> model<span class="token punctuation">.</span>most_similar<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'onion_rings|NOUN'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token punctuation">[</span><span class="token punctuation">(</span>u<span class="token string">'hashbrowns|NOUN'</span><span class="token punctuation">,</span> <span class="token number">0.8040812611579895</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'hot_dogs|NOUN'</span><span class="token punctuation">,</span> <span class="token number">0.7978234887123108</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'chicken_wings|NOUN'</span><span class="token punctuation">,</span> <span class="token number">0.793393611907959</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'sandwiches|NOUN'</span><span class="token punctuation">,</span> <span class="token number">0.7903584241867065</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'fries|NOUN'</span><span class="token punctuation">,</span> <span class="token number">0.7885469198226929</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'tater_tots|NOUN'</span><span class="token punctuation">,</span> <span class="token number">0.7821801900863647</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'bagels|NOUN'</span><span class="token punctuation">,</span> <span class="token number">0.7788236141204834</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'chicken_nuggets|NOUN'</span><span class="token punctuation">,</span> <span class="token number">0.7787706255912781</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'coleslaw|NOUN'</span><span class="token punctuation">,</span> <span class="token number">0.7771176099777222</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
 <span class="token punctuation">(</span>u<span class="token string">'nachos|NOUN'</span><span class="token punctuation">,</span> <span class="token number">0.7755396366119385</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</code></pre><p>Some of Reddit's ideas about food are kind of...interesting. It seems to think bacon and brocoll are very similar:</p><pre class="border-left border-thick mb4 o-block language-python"><code class="block font-sec lh2 px3 py3 smaller language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> model<span class="token punctuation">.</span>similarity<span class="token punctuation">(</span><span class="token string">'bacon|NOUN'</span><span class="token punctuation">,</span> <span class="token string">'broccoli|NOUN'</span><span class="token punctuation">)</span>
<span class="token number">0.83276615202851845</span>
</code></pre><p>Reddit also thinks hot dogs are practically salad:</p><pre class="border-left border-thick mb4 o-block language-python"><code class="block font-sec lh2 px3 py3 smaller language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> model<span class="token punctuation">.</span>similarity<span class="token punctuation">(</span><span class="token string">'hot_dogs|NOUN'</span><span class="token punctuation">,</span> <span class="token string">'salad|NOUN'</span><span class="token punctuation">)</span>
<span class="token number">0.76765100035460465</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> model<span class="token punctuation">.</span>similarity<span class="token punctuation">(</span><span class="token string">'hot_dogs|NOUN'</span><span class="token punctuation">,</span> <span class="token string">'entrails|NOUN'</span><span class="token punctuation">)</span>
<span class="token number">0.28360725445449464</span>
</code></pre><p>Just keep telling yourself that Reddit.</p><h2 class="mb3 lh1-25 h2"><a id="demo-usage" href="https://explosion.ai/blog/sense2vec-with-spacy#demo-usage" class="permalink relative"><svg aria-hidden="true" viewBox="0 0 24 24" width="24" height="24" class="align-middle permalink-icon"><use xlink:href="/assets/img/icons.svg#dots"></use></svg>Using the demo</a></h2><div class="bg-theme mb4 px3 py3 rounded"><h4 class="lh1-25 mb2 h4">Update (October 3, 2016)</h4><div class="small">We've updated the sense2vec demo, to allow you to specify the tags manually in the UI.
</div></div><p>Search for a word or phrase to explore related concepts.  If you want to get fancy, you can try adding a tag to your query, like so: <code>query phrase|NOUN</code>.  If you leave the tag out, we search for the most common one associated with the word.  The tags are predicted by a statistical model that looks at the surrounding context of each example of the word.</p><div class="lg-flex flex-wrap"><div class="lg-col fit lg-col-half"><h3 class="lh1-25 mb2 h3">Part-of-speech tags</h3><p><code>ADJ</code> <code>ADP</code> <code>ADV</code> <code>AUX</code> <code>CONJ</code> <code>DET</code> <code>INTJ</code> <code>NOUN</code> <code>NUM</code> <code>PART</code> <code>PRON</code> <code>PROPN</code> <code>PUNCT</code> <code>SCONJ</code> <code>SYM</code> <code>VERB</code> <code>X</code></p></div><div class="lg-col fit lg-col-half"><h3 class="lh1-25 mb2 h3">Named entities</h3><p><code>NORP</code> <code>FACILITY</code> <code>ORG</code> <code>GPE</code> <code>LOC</code> <code>PRODUCT</code> <code>EVENT</code> <code>WORK_OF_ART</code> <code>LANGUAGE</code></p></div></div><p>For instance, if you enter <code>serve</code>, we check how often many examples we have of <code>serve|VERB</code>, <code>serve|NOUN</code>, <code>serve|ADJ</code> etc.  Since <code>serve|VERB</code> is the most common, we show results for that query.  But if you type <code>serve|NOUN</code>, you can see results for thatinstead.  Because <code>serve|NOUN</code> is strongly associated with tennis, while usages of the verb are much more general, the results for the two queries are quite different.</p><p>We apply a similar frequency-based procedure to support case sensitivity.  If your query is lower case and has no tag, we assume it is case insensitive, and look for the most frequent tagged and cased version.  If your query includes at least one upper-case letter, or if you specify a tag, we assume you want the query to be case-sensitive.</p><div class="lg-flex flex-wrap items-center mt2"><a href="https://twitter.com/share?text=Sense2vec%20with%20spaCy%20and%20Gensim&amp;amp;via=explosion_ai" target="_blank" rel="noopener nofollow" class="a mr4" aria-label="Share on Twitter"><svg aria-hidden="true" viewBox="0 0 26 26" width="26" height="26" class="align-middle"><use xlink:href="/assets/img/icons.svg#twitter"></use></svg></a></div></section></article><section class="bg-theme px4 py4 xs-px1 angle angle-top-right angle-bottom-right"><div class="py2"><div class="lg-flex flex-wrap justify-center"><div class="lg-col fit lg-col-3 pr3"><img src="./Sense2vec with spaCy and Gensim _ Blog _ Explosion AI_files/profile_matt.jpg" alt="Matthew Honnibal" width="250" class="circle"></div><div class="lg-col fit lg-col-7"><span class="small font-sec upper">About the Author</span><h3 class="lh1-25 mb2 h4">Matthew Honnibal</h3><div class="small mb3">Matthew is a leading expert in AI technology, known for his research, software and writings. He completed his PhD in 2009, and spent a further 5 years publishing research on state-of-the-art natural language understanding systems. Anticipating the AI boom, he left academia in 2014 to develop spaCy, an open-source library for industrial-strength NLP.</div><div class="block"><a href="mailto:matt@explosion.ai" class="a mr2" aria-label="Email"><svg aria-hidden="true" viewBox="0 0 24 24" width="24" height="24" class="align-middle"><use xlink:href="/assets/img/icons.svg#mail"></use></svg></a><a href="https://twitter.com/honnibal" target="_blank" rel="noopener nofollow" class="a mr2" aria-label="twitter"><svg aria-hidden="true" viewBox="0 0 24 24" width="24" height="24" class="align-middle"><use xlink:href="/assets/img/icons.svg#twitter"></use></svg></a><a href="https://github.com/honnibal" target="_blank" rel="noopener nofollow" class="a mr2" aria-label="github"><svg aria-hidden="true" viewBox="0 0 24 24" width="24" height="24" class="align-middle"><use xlink:href="/assets/img/icons.svg#github"></use></svg></a><a href="https://www.semanticscholar.org/search?q=Matthew%20Honnibal" target="_blank" rel="noopener nofollow" class="a mr2" aria-label="semanticscholar"><svg aria-hidden="true" viewBox="0 0 24 24" width="24" height="24" class="align-middle"><use xlink:href="/assets/img/icons.svg#semanticscholar"></use></svg></a></div></div></div></div></section><section class="fit bg z1 px4 xs-px1 pt3 pb4"><h3 class="lh1-25 mb2 h3 slider-head">Read more</h3><div class="js-slider slider relative nowrap fit mt2 js-slider-undefined"><div class="js-slider-frame flex fit relative overflow-hidden"><div class="js-slider-slides slides fit inline-block" style="transition-timing-function: ease; transition-duration: 600ms; transform: translate3d(0px, 0px, 0px);">
<div class="lg-col pr2 mb2 relative reset-nowrap lg-col-3 active"><a href="https://explosion.ai/blog/supervised-learning-data-collection" class="a block height-full"><div style="background: #e76d5d" class="flex flex-column py3 px3 height-full bg-theme-light relative rounded color-light"><h3 class="lh1-25 mb2 h5">Supervised learning is great — it's data collection that's broken</h3><div class="smaller"><time datetime="2017-04-02T00:00:00.000Z">April 2, 2017</time></div></div></a></div><div class="lg-col pr2 mb2 relative reset-nowrap lg-col-3"><a href="https://explosion.ai/blog/supervised-similarity-siamese-cnn" class="a block height-full"><div style="background: #9a9ea2" class="flex flex-column py3 px3 height-full bg-theme-light relative rounded color-light"><h3 class="lh1-25 mb2 h5">Supervised similarity: Learning symmetric relations from duplicate question data</h3><div class="smaller"><time datetime="2017-03-01T00:00:00.000Z">March 1, 2017</time></div></div></a></div><div class="lg-col pr2 mb2 relative reset-nowrap lg-col-3"><a href="https://explosion.ai/blog/quora-deep-text-pair-classification" class="a block height-full"><div style="background: #b92c27" class="flex flex-column py3 px3 height-full bg-theme-light relative rounded color-light"><h3 class="lh1-25 mb2 h5">Deep text-pair classification with Quora's 2017 question dataset</h3><div class="smaller"><time datetime="2017-02-13T00:00:00.000Z">February 13, 2017</time></div></div></a></div><div class="lg-col pr2 mb2 relative reset-nowrap lg-col-3"><a href="https://explosion.ai/blog/deep-learning-formula-nlp" class="a block height-full"><div style="background: #5e539f" class="flex flex-column py3 px3 height-full bg-theme-light relative rounded color-light"><h3 class="lh1-25 mb2 h5">Embed, encode, attend, predict: The new deep learning formula for state-of-the-art NLP models</h3><div class="smaller"><time datetime="2016-11-10T00:00:00.000Z">November 10, 2016</time></div></div></a></div><div class="lg-col pr2 mb2 relative reset-nowrap lg-col-3"><a href="https://explosion.ai/blog/spacy-user-survey" class="a block height-full"><div style="background: #d9515d" class="flex flex-column py3 px3 height-full bg-theme-light relative rounded color-light"><h3 class="lh1-25 mb2 h5">The spaCy user survey: results and analysis</h3><div class="smaller"><time datetime="2016-11-07T00:00:00.000Z">November 7, 2016</time></div></div></a></div><div class="lg-col pr2 mb2 relative reset-nowrap lg-col-3"><a href="https://explosion.ai/blog/chatbot-node-js-spacy" class="a block height-full"><div style="background: #24272e" class="flex flex-column py3 px3 height-full bg-theme-light relative rounded color-light"><h3 class="lh1-25 mb2 h5">Building your bot's brain with Node.js and spaCy</h3><div class="smaller"><time datetime="2016-10-23T00:00:00.000Z">October 23, 2016</time></div></div></a></div><div class="lg-col pr2 mb2 relative reset-nowrap lg-col-3"><a href="https://explosion.ai/blog/spacy-deep-learning-keras" class="a block height-full"><div style="background: #3da4d5" class="flex flex-column py3 px3 height-full bg-theme-light relative rounded color-light"><h3 class="lh1-25 mb2 h5">spaCy v1.0: Deep Learning with custom pipelines and Keras</h3><div class="smaller"><time datetime="2016-10-19T00:00:00.000Z">October 19, 2016</time></div></div></a></div><div class="lg-col pr2 mb2 relative reset-nowrap lg-col-3"><a href="https://explosion.ai/blog/displacy-ent-named-entity-visualizer" class="a block height-full"><div style="background: #607d8b" class="flex flex-column py3 px3 height-full bg-theme-light relative rounded color-light"><h3 class="lh1-25 mb2 h5">An open-source named entity visualiser for the modern web</h3><div class="smaller"><time datetime="2016-10-05T00:00:00.000Z">October 5, 2016</time></div></div></a></div><div class="lg-col pr2 mb2 relative reset-nowrap lg-col-3"><a href="https://explosion.ai/blog/introducing-explosion-ai" class="a block height-full"><div style="background: #1e1a35" class="flex flex-column py3 px3 height-full bg-theme-light relative rounded color-light"><h3 class="lh1-25 mb2 h5">Introducing Explosion AI</h3><div class="smaller"><time datetime="2016-10-03T00:00:00.000Z">October 3, 2016</time></div></div></a></div><div class="lg-col pr2 mb2 relative reset-nowrap lg-col-3"><a href="https://explosion.ai/blog/displacy-js-nlp-visualizer" class="a block height-full"><div style="background: #514dfb" class="flex flex-column py3 px3 height-full bg-theme-light relative rounded color-light"><h3 class="lh1-25 mb2 h5">displaCy.js: An open-source NLP visualiser for the modern web</h3><div class="smaller"><time datetime="2016-10-03T00:00:00.000Z">October 3, 2016</time></div></div></a></div></div></div><button aria-label="prev" class="slide-nav absolute js-slider-prev prev hov-opacity"><svg aria-hidden="true" viewBox="0 0 30 30" width="30" height="30" class="align-middle slide-nav-icon"><use xlink:href="/assets/img/icons.svg#caret-left"></use></svg></button><button aria-label="next" class="slide-nav absolute js-slider-next next hov-opacity"><svg aria-hidden="true" viewBox="0 0 30 30" width="30" height="30" class="align-middle slide-nav-icon"><use xlink:href="/assets/img/icons.svg#caret-right"></use></svg></button></div></section></main>
<footer><section class="bg-theme px4 py4 xs-px1 angle angle-top-right"><div class="py2"><div class="lg-flex flex-wrap justify-between small"><div class="lg-col fit lg-col-2 px2 mb3"><a href="https://explosion.ai/" aria-label="Home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 500 500" width="40" height="40" fill="currentColor" style="width: 3.5rem; height: 3.5rem" class="hov-rotate"><path d="M111.7 74.9L91.2 93.1l9.1 10.2 17.8-15.8 7.4 8.4-17.8 15.8 10.1 11.4 20.6-18.2 7.7 8.7-30.4 26.9-41.9-47.3 30.3-26.9 7.6 8.6zM190.8 59.6L219 84.3l-14.4 4.5-20.4-18.2-6.4 26.6-14.4 4.5 8.9-36.4-26.9-24.1 14.3-4.5L179 54.2l5.7-25.2 14.3-4.5-8.2 35.1zM250.1 21.2l27.1 3.4c6.1.8 10.8 3.1 14 7.2 3.2 4.1 4.5 9.2 3.7 15.5-.8 6.3-3.2 11-7.4 14.1-4.1 3.1-9.2 4.3-15.3 3.5L258 63.2l-2.8 22.3-13-1.6 7.9-62.7zm11.5 13l-2.2 17.5 12.6 1.6c5.1.6 9.1-2 9.8-7.6.7-5.6-2.5-9.2-7.6-9.9l-12.6-1.6zM329.1 95.4l23.8 13.8-5.8 10L312 98.8l31.8-54.6 11.3 6.6-26 44.6zM440.5 145c-1.3 8.4-5.9 15.4-13.9 21.1s-16.2 7.7-24.6 6.1c-8.4-1.6-15.3-6.3-20.8-14.1-5.5-7.9-7.6-16-6.4-24.4 1.3-8.5 6-15.5 14-21.1 8-5.6 16.2-7.7 24.5-6 8.4 1.6 15.4 6.3 20.9 14.2 5.5 7.6 7.6 15.7 6.3 24.2zM412 119c-5.1-.8-10.3.6-15.6 4.4-5.2 3.7-8.4 8.1-9.4 13.2-1 5.2.2 10.1 3.5 14.8 3.4 4.8 7.5 7.5 12.7 8.2 5.2.8 10.4-.7 15.6-4.4 5.3-3.7 8.4-8.1 9.4-13.2 1.1-5.1-.1-9.9-3.4-14.7-3.4-4.8-7.6-7.6-12.8-8.3zM471.5 237.9c-2.8 4.8-7.1 7.6-13 8.7l-2.6-13.1c5.3-.9 8.1-5 7.2-11-.9-5.8-4.3-8.8-8.9-8.2-2.3.3-3.7 1.4-4.5 3.3-.7 1.9-1.4 5.2-1.7 10.1-.8 7.5-2.2 13.1-4.3 16.9-2.1 3.9-5.7 6.2-10.9 7-6.3.9-11.3-.5-15.2-4.4-3.9-3.8-6.3-9-7.3-15.7-1.1-7.4-.2-13.7 2.6-18.8 2.8-5.1 7.4-8.2 13.7-9.2l2.6 13c-5.6 1.1-8.7 6.6-7.7 13.4 1 6.6 3.9 9.5 8.6 8.8 4.4-.7 5.7-4.5 6.7-14.1.3-3.5.7-6.2 1.1-8.4.4-2.2 1.2-4.4 2.2-6.8 2.1-4.7 6-7.2 11.8-8.1 5.4-.8 10.3.4 14.5 3.7 4.2 3.3 6.9 8.5 8 15.6.9 6.9-.1 12.6-2.9 17.3zM408.6 293.5l2.4-12.9 62 11.7-2.4 12.9-62-11.7zM419.6 396.9c-8.3 2-16.5.3-24.8-5-8.2-5.3-13.2-12.1-14.9-20.5-1.6-8.4.1-16.6 5.3-24.6 5.2-8.1 11.9-13.1 20.2-15.1 8.4-1.9 16.6-.3 24.9 5 8.2 5.3 13.2 12.1 14.8 20.5 1.7 8.4 0 16.6-5.2 24.7-5.2 8-12 13-20.3 15zm13.4-36.3c-1.2-5.1-4.5-9.3-9.9-12.8s-10.6-4.7-15.8-3.7-9.3 4-12.4 8.9-4.1 9.8-2.8 14.8c1.2 5.1 4.5 9.3 9.9 12.8 5.5 3.5 10.7 4.8 15.8 3.7 5.1-.9 9.2-3.8 12.3-8.7s4.1-9.9 2.9-15zM303.6 416.5l9.6-5.4 43.3 20.4-19.2-34 11.4-6.4 31 55-9.6 5.4-43.4-20.5 19.2 34.1-11.3 6.4-31-55zM238.2 468.8c-49 0-96.9-17.4-134.8-49-38.3-32-64-76.7-72.5-125.9-2-11.9-3.1-24-3.1-35.9 0-36.5 9.6-72.6 27.9-104.4 2.1-3.6 6.7-4.9 10.3-2.8 3.6 2.1 4.9 6.7 2.8 10.3-16.9 29.5-25.9 63.1-25.9 96.9 0 11.1 1 22.3 2.9 33.4 7.9 45.7 31.8 87.2 67.3 116.9 35.2 29.3 79.6 45.5 125.1 45.5 11.1 0 22.3-1 33.4-2.9 4.1-.7 8 2 8.7 6.1.7 4.1-2 8-6.1 8.7-11.9 2-24 3.1-36 3.1z"></path></svg></a></div><div class="lg-col fit lg-col-2 px2 mb3 xs-hide sm-hide md-hide"><span class="small font-sec upper block mb2">Navigation</span><ul class="smaller"><li><a href="https://explosion.ai/">Home</a></li><li><a href="https://explosion.ai/about">About us</a></li><li><a href="https://explosion.ai/blog" class="bold">Blog</a></li><li><a href="https://explosion.ai/request">Request</a></li><li><a href="https://explosion.ai/legal">Legal / Imprint</a></li></ul></div><div class="lg-col fit lg-col-4 px2 mb3"><span class="small font-sec upper block mb2">Explosion AI</span><div class="smaller mb2">Explosion AI is a digital studio specialising in Artificial Intelligence and Natural Language Processing. We're the makers of spaCy, the leading open-source NLP library.</div><div class="block"><a href="mailto:contact@explosion.ai" class="a mr2" aria-label="Email"><svg aria-hidden="true" viewBox="0 0 20 20" width="20" height="20" class="align-middle"><use xlink:href="/assets/img/icons.svg#mail"></use></svg></a><a href="https://twitter.com/explosion_ai" target="_blank" rel="noopener nofollow" class="a mr2" aria-label="twitter"><svg aria-hidden="true" viewBox="0 0 20 20" width="20" height="20" class="align-middle"><use xlink:href="/assets/img/icons.svg#twitter"></use></svg></a><a href="https://github.com/explosion" target="_blank" rel="noopener nofollow" class="a mr2" aria-label="github"><svg aria-hidden="true" viewBox="0 0 20 20" width="20" height="20" class="align-middle"><use xlink:href="/assets/img/icons.svg#github"></use></svg></a><a href="https://codepen.io/explosion" target="_blank" rel="noopener nofollow" class="a mr2" aria-label="codepen"><svg aria-hidden="true" viewBox="0 0 20 20" width="20" height="20" class="align-middle"><use xlink:href="/assets/img/icons.svg#codepen"></use></svg></a><a href="https://explosion.ai/feed" class="a mr2" aria-label="feed"><svg aria-hidden="true" viewBox="0 0 20 20" width="20" height="20" class="align-middle"><use xlink:href="/assets/img/icons.svg#feed"></use></svg></a><a href="https://explosion.ai/legal" class="a smallest lg-hide">Legal / Imprint</a></div></div><div class="lg-col fit lg-col-4 px2 mb3"><span class="small font-sec upper block mb2">Stay in the loop</span><div class="smaller mb2">Join our mailing list to receive updates about new blog posts and projects.</div><div class="pr4">
<div id="mc_embed_signup"><form id="mc-embedded-subscribe-form" action="https://spacy.us12.list-manage.com/subscribe/post?u=83b0498b1e7fa3c91ce68c3f1&amp;amp;id=3b8b243b6e" method="post" name="mc-embedded-subscribe-form" target="_blank" novalidate="" class="validate"><div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_83b0498b1e7fa3c91ce68c3f1_3b8b243b6e" tabindex="-1" value=""></div><div class="flex bg-theme-light rounded smaller pr2 pl1"><input id="mce-EMAIL" type="email" name="EMAIL" value="" placeholder="Your email address" spellcheck="false" style="width: 100%" class="px1 py1"><button id="mc-embedded-subscribe" type="submit" name="subscribe" aria-label="Subscribe" class="hov-move-right"><svg aria-hidden="true" viewBox="0 0 24 24" width="24" height="24" class="align-middle"><use xlink:href="/assets/img/icons.svg#arrow-right"></use></svg></button></div></form></div></div></div></div></div></section></footer><script src="./Sense2vec with spaCy and Gensim _ Blog _ Explosion AI_files/lory.js"></script><script src="./Sense2vec with spaCy and Gensim _ Blog _ Explosion AI_files/main.js"></script><script src="./Sense2vec with spaCy and Gensim _ Blog _ Explosion AI_files/prism.js"></script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga('create', 'UA-84293596-1', 'auto'); ga('send', 'pageview');</script><script async="" src="./Sense2vec with spaCy and Gensim _ Blog _ Explosion AI_files/analytics.js"></script></body></html>